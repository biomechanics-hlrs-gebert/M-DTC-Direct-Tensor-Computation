#!/bin/bash
# ------------------------------------------------------------------------------
# Batch scipt for HLRS Hawk
#
# Author:    Johannes Gebert - HLRS - NUM - gebert@hlrs.de
# Date:      09.01.2022
# Last edit: 03.12.2022
#
# For use of PBSPro visit:
# https://kb.hlrs.de/platforms/index.php/Batch_System_PBSPro_(Hawk)
# ------------------------------------------------------------------------------
# IF USED FOR LOAD BALANCING: TAKE CARE NOT TO ALLOCATE TOO MANY NODES!
# TASK_GEOMETRY ONLY WORKS WITH MPT. NOT WITH Open MPI
# ------------------------------------------------------------------------------
#PBS -N fh01-96
#PBS -l select=512:node_type=rome:mpiprocs=128:node_type_mem=256gb
#PBS -l walltime=24:00:00
####PBS -q test
#PBS -M gebert@hlrs.de
NO_NODES=512
PROCESSORS_PER_NODE=128
BINARY="./bin/dtc_V1.1.0-p_x86_64"
DATASET_DIR="datasets/"
#
try_how_many_times=10
#
# Set the basename of the datasets, according to the MeRaDat format.
declare -a basenames=(\
   "FH01-1_mu_Dev_dtc_Tensors-96-004088" \
   "FH01-1_mu_Dev_dtc_Tensors-96-008176" \
   "FH01-1_mu_Dev_dtc_Tensors-96-016352" \
   "FH01-1_mu_Dev_dtc_Tensors-96-032704" \
   )
# The number of processors used by MPI
declare -a processors=(\
   "4089" \
   "8177" \
   "16353" \
   "32705" \
    )
# Parts per domain. Calc by hand too.
declare -a ppd=(\
   "4088" \
   "8176" \
   "16352" \
   "32704" \
    )
# Nodes per domain. Calc by hand too.
declare -a npd=(\
   "511" \
   "511" \
   "511" \
   "511" \
    )
#    
# USER INPUT COMPLETE ==========================================================
# ==============================================================================
# JOBS START ===================================================================
#
# ------------------------------------------------------------------------------
# Change to the directory that the job was submitted from and 
# Source the environment. »shellcheck« is some sort of a pragma for shellcheck (!)
# ------------------------------------------------------------------------------
cd "$PBS_O_WORKDIR" || exit
#
# ------------------------------------------------------------------------------
# Get the timestamp in Milliseconds as in the ELK-Stack. 
# Parse with something like 'grep "JOB_START" | cut -d "=" -f 2'
# ------------------------------------------------------------------------------
echo "TimeStampMillis JOB_START=$(($(date +%s) * 1000))"
echo "##############################################START#################################"
echo "Job details $(qstat -f)"
#
# ------------------------------------------------------------------------------
# Create a list of the nodes engaged in the batch job.
# ------------------------------------------------------------------------------
id_short=$(echo "$PBS_JOBID" | cut -d "." -f 1)
#
awk '!seen[$0]++' "$PBS_NODEFILE" > "$PBS_JOBNAME.$id_short.nodefile"
#
# shellcheck source=/dev/null
source "./environment.source" hawk --no-output
#
# -----------------------------------------------------------------------------
# Python is a requirement special to DTC.
# ------------------------------------------------------------------------------
module load python || exit
#
#
tracker_file="${PBS_JOBNAME}.i${PBS_JOBID}.batch_tracker"
try_counter=0
for ((ii=0; ii<${#basenames[@]}; ii++));
do
    #
    # ------------------------------------------------------------------------------
    # Workaround for creating the proper directories.
    # No need for '|| continue' if it fails, the DTC will try by itself.
    # ------------------------------------------------------------------------------
    script="auxiliaries/create_directories.DTC.hawk.py"
    #
    if [ -f "$script" ]
    then
        python "$script" \
            -p_n_bsnm "$DATASET_DIR/${basenames[ii]}" \
            -worker_bsnm Rank \
            -ppd "${ppd[ii]}" \
            -size_mpi "${processors[ii]}"
    else
        echo "The script $script was not found!"
    fi
    #
    # ------------------------------------------------------------------------------
    # Create the *.hostfile file; Skip basename if python does not suceed.
    # ------------------------------------------------------------------------------
    host_file="$DATASET_DIR/${basenames[ii]}.hostfile"
    script="auxiliaries/write_hostfile.npd.DTC.hawk.py"
    #
    if [ -f "$script" ]
    then
    python "$script" \
        -if="$DATASET_DIR/${basenames[ii]}.meta" \
        -npd="${npd[ii]}" \
        -ppd="${ppd[ii]}" \
        -nodelist="$PBS_JOBNAME.$id_short.nodefile" \
        -hf="$host_file" || continue
    else
        echo "The script $script was not found!"
    fi
    #
    # ------------------------------------------------------------------------------
    # Start memlogging
    # ------------------------------------------------------------------------------
    "$DATASET_DIR"/memlog.sh "$DATASET_DIR/${basenames[ii]}".memlog > /dev/null 2> /dev/null &
    #
    # ------------------------------------------------------------------------------
    # Send variable to DTC
    # ------------------------------------------------------------------------------
    export BATCH_ORDER="$ii"
    #
    # ------------------------------------------------------------------------------
    # Parametrize DTC and run it.
    # ------------------------------------------------------------------------------
    #
    echo "Computing: $DATASET_DIR/${basenames[ii]}.meta"
    mpirun -np "${processors[ii]}" \
        -hostfile "$host_file" \
        --nooversubscribe \
        "$BINARY" \
        "$PBS_O_WORKDIR" \
        "$PWD/$DATASET_DIR/${basenames[ii]}".meta \
           >>"$DATASET_DIR/${basenames[ii]}".std_out \
          2>>"$DATASET_DIR/${basenames[ii]}".std_err
    # --report-bindings \
    #
    # ------------------------------------------------------------------------------
    # Restart the application up to $how_many_times if the application crashed
    # ------------------------------------------------------------------------------
    if [ $try_counter -eq $try_how_many_times ]
    then
        #
        # ------------------------------------------------------------------------------
        # Reset after n times
        # ------------------------------------------------------------------------------
        try_counter=0
    else
        if [ -f "BATCH_RUN" ]
        then
            BATCH_RUN=$(<BATCH_RUN)
        else
            BATCH_RUN="NOT_SET"
        fi
        #
        if [ "$BATCH_RUN" = "JOB_FINISHED" ]
        then
            echo "TimeStampMillis ${basenames[ii]}-END=$(($(date +%s) * 1000))"
            #
            # ------------------------------------------------------------------------------
            # User feedback
            # ------------------------------------------------------------------------------
            {
            echo "$DATASET_DIR/${basenames[ii]}.meta finished"; 
            echo ""
            } >> "$tracker_file"
            #
            # ------------------------------------------------------------------------------
            # (Re)Set counters
            # ------------------------------------------------------------------------------
            try_counter=0
            # 
            # ------------------------------------------------------------------------------
            # Crawl results, single threaded. Not efficient, but reduces the out amount.
            # Continue statement is important to avoid data corruption.
            # ------------------------------------------------------------------------------
            ./bin/meRaDat_Crawl_Tensors_x86_64 "$DATASET_DIR/${basenames[ii]}.meta" >/dev/null 2>/dev/null || continue
            #
            # ------------------------------------------------------------------------------
            # Move mem-/hardware logs to the root directory of the output data.
            # Continue statement is important to avoid data corruption.
            # ------------------------------------------------------------------------------
            mv "$DATASET_DIR/${basenames[ii]}/"*"/"*".memlog" "$DATASET_DIR/${basenames[ii]}/" || continue
            #
            # ------------------------------------------------------------------------------
            # Delete unnecessary result data.
            # DO NOT interchange this delete with the preceeding mv command.
            # ------------------------------------------------------------------------------
            rm -r "$DATASET_DIR/${basenames[ii]}/Rank_"*
            #
            # ------------------------------------------------------------------------------
            # Optimize the tensors
            # ------------------------------------------------------------------------------
            ROT_BINARY="rot/bin/rot__x86_64"
            ROT_TEMPLATE="$DATASET_DIR/M-ROT.meta.template"
            #
            echo "TimeStampMillis ${basenames[ii]}-ROT-START=$(($(date +%s) * 1000))"
            #
            if [ -f "$ROT_BINARY" ] && [ -f "$ROT_TEMPLATE" ]
            then
                cat "$ROT_TEMPLATE" >> "$DATASET_DIR/${basenames[ii]}.meta"
                #
                mpirun -np $(("$NO_NODES"*"$PROCESSORS_PER_NODE")) "$ROT_BINARY" "$DATASET_DIR/${basenames[ii]}.meta" >/dev/null 2>/dev/null || continue
                #
                echo "TimeStampMillis ${basenames[ii]}-ROT-END=$(($(date +%s) * 1000))"
                #
            else
                echo "Tensors for ${basenames[ii]} have not been optimized due to an exception regarding the rot template or binary."
            fi 
            #
        else
            {
            echo "BATCH_RUN=$BATCH_RUN"; 
            echo ""
            } >> "$tracker_file"
            #
            ii=$((ii-1))
            try_counter=$((try_counter+1))
        fi
    fi
done
#
if [ -f "BATCH_RUN" ]
then
    rm BATCH_RUN
fi 
#
# ------------------------------------------------------------------------------
# Get the timestamp in Milliseconds as in the ELK-Stack. 
# Parse with something like 'grep "JOB_END" | cut -d "=" -f 2'
# ------------------------------------------------------------------------------
echo "TimeStampMillis JOB_END=$(($(date +%s) * 1000))"