#!/bin/bash
# ------------------------------------------------------------------------------
# Batch scipt for HLRS Hawk
#
# Author:    Johannes Gebert - HLRS - NUM - gebert@hlrs.de
# Date:      09.01.2022
# Last edit: 29.11.2022
#
# For use of PBSPro visit:
# https://kb.hlrs.de/platforms/index.php/Batch_System_PBSPro_(Hawk)
# ------------------------------------------------------------------------------
# IF USED FOR LOAD BALANCING: TAKE CARE NOT TO ALLOCATE TOO MANY NODES!
# TASK_GEOMETRY ONLY WORKS WITH MPT. NOT WITH Open MPI
# ------------------------------------------------------------------------------
#PBS -N fh01-pow
#PBS -l select=8:node_type=rome:mpiprocs=128:node_type_mem=256gb
#PBS -l walltime=00:25:00
#PBS -q test
#PBS -M gebert@hlrs.de
NO_NODES=8
PROCESSORS_PER_NODE=128
# ------------------------------------------------------------------------------
#
# ------------------------------------------------------------------------------
# Change to the directory that the job was submitted from and 
# Source the environment. »shellcheck« is some sort of a pragma for shellcheck (!)
# ------------------------------------------------------------------------------
cd "$PBS_O_WORKDIR" || exit
#
# ------------------------------------------------------------------------------
# Get the timestamp in Milliseconds as in the ELK-Stack. 
# Parse with something like 'grep "JOB_START" | cut -d "=" -f 2'
# ------------------------------------------------------------------------------
echo "TimeStampMillis JOB_START=$(($(date +%s) * 1000))"
echo "###############################################################################"
echo "Job details $(qstat -f)"
#
# ------------------------------------------------------------------------------
# Create a list of the nodes engaged in the batch job.
# ------------------------------------------------------------------------------
id_short=$(echo "$PBS_JOBID" | cut -d "." -f 1)
#
# awk '!seen[$0]++' "$PBS_NODEFILE" | cut -d "." -f 1 > "$PBS_JOBNAME.$id_short.nodefile"
awk '!seen[$0]++' "$PBS_NODEFILE" > "$PBS_JOBNAME.$id_short.nodefile"
#
# shellcheck source=/dev/null
source "./environment.source" hawk --no-output
#
# ------------------------------------------------------------------------------
# Python required for writing the *.task_geometry files
# ------------------------------------------------------------------------------
module load python || exit
#
# ------------------------------------------------------------------------------
# Set the binary and the dataset directory.
# ------------------------------------------------------------------------------
BINARY="./bin/dtc_V1.1.0-p_x86_64"
DATASET_DIR="datasets/"
#
try_how_many_times=5
#
# ------------------------------------------------------------------------------
# Set the basename of the datasets, according to the MeRaDat format.
# In the second array, declare the number of processors used.
# ------------------------------------------------------------------------------
# The number of processors is required for mpi to cross-check the allocation 
# of compute resources
# The number of parts per node is required for power and energy measurements
# to properly pin domains to nodes.
# ------------------------------------------------------------------------------
declare -a basenames=(\
   "FH01-1_mu_Dev_dtc_Tensors182" \
   "FH01-1_mu_Dev_dtc_Tensors90" \
   "FH01-1_mu_Dev_dtc_Tensors45" \
   "FH01-1_mu_Dev_dtc_Tensors35" \
   )
# Calculate them by hand, it is a good quality measure.
declare -a processors=(\
   "547" \
   "631" \
   "316" \
   "246" \
    )
# ------------------------------------------------------------------------------
tracker_file="${PBS_JOBNAME}.i${PBS_JOBID}.batch_tracker"
try_counter=0
for ((ii=0; ii<${#basenames[@]}; ii++));
do
    #
    # ------------------------------------------------------------------------------
    # Create the *.task_geometry file; Skip basename if python does not suceed.
    # ------------------------------------------------------------------------------
    host_file="$DATASET_DIR/${basenames[ii]}.hostfile"
    #
    python ./auxiliaries/write_hostfile.DTC.hawk.py \
        -if="$DATASET_DIR/${basenames[ii]}.meta" \
        -nodes="$NO_NODES" \
        -ppn="$PROCESSORS_PER_NODE" \
        -nodelist="$PBS_JOBNAME.$id_short.nodefile" \
        -hf="$host_file" || continue
    #
    # ------------------------------------------------------------------------------
    # Start memlogging
    # ------------------------------------------------------------------------------
    "$DATASET_DIR"/memlog.sh "$DATASET_DIR/${basenames[ii]}".memlog > /dev/null 2> /dev/null &
    #
    # ------------------------------------------------------------------------------
    # User feedback
    # ------------------------------------------------------------------------------
    echo "$DATASET_DIR/${basenames[ii]}.meta started" >> "$tracker_file"
    #
    # ------------------------------------------------------------------------------
    # Send variable to DTC
    # ------------------------------------------------------------------------------
    export BATCH_ORDER="$ii"
    #
    # ------------------------------------------------------------------------------
    # Parametrize the program and run it
    # ppn = Parts per node - how many processes per node, dpending on part per domain
    # mpiprocs=128 -> default by PBS; how many processes per node
    # ------------------------------------------------------------------------------
    #
    echo "Computing: $DATASET_DIR/${basenames[ii]}.meta"
    mpirun -np "${processors[ii]}" -hostfile "$host_file" \
            --report-bindings --nooversubscribe \
            "$BINARY" "$PBS_O_WORKDIR" "$PWD/$DATASET_DIR/${basenames[ii]}".meta \
            >>"$DATASET_DIR/${basenames[ii]}".std_out 2>>"$DATASET_DIR/${basenames[ii]}".std_err
    # 
	# job_name.i[nfo]job_id.batch_tracker${PBS_JOBNAME}
    #
    # ------------------------------------------------------------------------------
    # Restart the application up to $how_many_times if the application crashed
    # ------------------------------------------------------------------------------
    if [ $try_counter -eq $try_how_many_times ]
    then
        #
        # ------------------------------------------------------------------------------
        # Reset after n times
        # ------------------------------------------------------------------------------
        try_counter=0
    else
        if [ -f "BATCH_RUN" ]
        then
            BATCH_RUN=$(<BATCH_RUN)
        else
            BATCH_RUN="NOT_SET"
        fi
        #
        if [ "$BATCH_RUN" = "JOB_FINISHED" ]
        then
            #
            # ------------------------------------------------------------------------------
            # User feedback
            # ------------------------------------------------------------------------------
            {
            echo "$DATASET_DIR/${basenames[ii]}.meta finished"; 
            echo ""
            } >> "$tracker_file"
            #
            # ------------------------------------------------------------------------------
            # (Re)Set counters
            # ------------------------------------------------------------------------------
           try_counter=0
        else
            {
            echo "BATCH_RUN=$BATCH_RUN"; 
            echo ""
            } >> "$tracker_file"
            #
            ii=$((ii-1))
            try_counter=$((try_counter+1))
        fi
    fi
done
#
if [ -f "BATCH_RUN" ]
then
    rm BATCH_RUN
fi 
#
# ------------------------------------------------------------------------------
# Get the timestamp in Milliseconds as in the ELK-Stack. 
# Parse with something like 'grep "JOB_END" | cut -d "=" -f 2'
# ------------------------------------------------------------------------------
echo "TimeStampMillis JOB_END=$(($(date +%s) * 1000))"